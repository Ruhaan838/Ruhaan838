# üëã Hi there! I'm **Ruhaan**

A passionate **student** exploring the world of **AI, ML, DL**, and **GPU programming** with a strong focus on **PyTorch**, **CUDA C++**, and **Reinforcement Learning**.

üåê [Visit My Portfolio](https://ruhaan838.github.io/)

---

## Featured Projects

<table>
  <thead>
    <tr>
      <th>Project</th>
      <th>Description</th>
      <th>Links</th>
      <th>Tech Used</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Flash Attention (CUDA)</strong></td>
      <td>Forward & Backward pass of Flash Attention in CUDA</td>
      <td>
        <a href="https://github.com/Ruhaan838/100Day-GPU/tree/main/Day-10">Forward</a>,
        <a href="https://github.com/Ruhaan838/100Day-GPU/blob/main/Day-17/flash_att_backward.cu">Backward</a>
      </td>
      <td>CUDA C++</td>
    </tr>
    <tr>
      <td><strong>Conv1D & Conv2D (CUDA)</strong></td>
      <td>Shared & tiled memory implementations of Conv1D/2D</td>
      <td><a href="https://github.com/Ruhaan838/100Day-GPU/tree/main/Day-07">Link</a></td>
      <td>CUDA C++</td>
    </tr>
    <tr>
      <td><strong>HIP DL Algorithms</strong></td>
      <td>14+ DL and linear algebra kernels using HIP/rocBLAS</td>
      <td><a href="https://github.com/Ruhaan838/100Day-GPU/tree/main/Day-30">Link</a></td>
      <td>HIP C++</td>
    </tr>
    <tr>
      <td><strong>LLaMA 2 (PyTorch)</strong></td>
      <td>LLaMA2 from scratch with training & inference loops</td>
      <td><a href="https://github.com/Ruhaan838/LLaMA-2-pytorch">Link</a></td>
      <td>Python, PyTorch</td>
    </tr>
    <tr>
      <td><strong>AnyGrad</strong></td>
      <td>Minimal tensor library in C++ with Python bindings</td>
      <td><a href="https://github.com/Ruhaan838/AnyGrad.git">Link</a></td>
      <td>C++, Python</td>
    </tr>
  </tbody>
</table>

---

## Currently Learning

- **Learning CUDA in 100 Days**  
  <a href="https://github.com/Ruhaan838/100Day-GPU">github.com/Ruhaan838/100Day-GPU</a>

- **Learning Reinforcement Learning in 100 Days**  
  *(Private Repository)*

---

## Tech Stack

<table>
  <tr>
    <td align="center" width="120">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg" width="48" height="48" alt="Python"><br>Python
    </td>
    <td align="center" width="120">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/cplusplus/cplusplus-original.svg" width="48" height="48" alt="C++"><br>C++
    </td>
    <td align="center" width="120">
    <img src="https://www.vectorlogo.zone/logos/nvidia/nvidia-ar21.svg" width="98" height="48" alt="CUDA"><br>CUDA
    </td>
    <td align="center" width="120">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pytorch/pytorch-original.svg" width="48" height="48" alt="PyTorch"><br>PyTorch
    </td>
  </tr>
  <tr>
    <td align="center" width="120">
      <img src="https://www.vectorlogo.zone/logos/amd/amd-ar21.svg" width="98" height="48" alt="HIP"><br>HIP / ROCm
    </td>
    <td align="center" width="120">
      <img src="https://resources.jetbrains.com/storage/products/clion/img/meta/clion_logo_300x300.png" width="48" height="48" alt="CLion"><br>CLion
    </td>
    <td align="center" width="120">
      <img src="https://resources.jetbrains.com/storage/products/pycharm/img/meta/pycharm_logo_300x300.png" width="48" height="48" alt="PyCharm"><br>PyCharm
    </td>
    <td align="center" width="120">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg" width="48" height="48" alt="Linux"><br>Linux
    </td>
  </tr>
</table>


---

## GitHub Stats

<table>
  <tr>
    <td>
      <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=Ruhaan838&layout=compact&theme=radical" alt="Top Langs">
    </td>
    <td>
      <img src="https://streak-stats.demolab.com/?user=Ruhaan838&theme=radical" alt="GitHub Streak">
    </td>
    <td>
      ![Contribution Snake](https://raw.githubusercontent.com/Ruhaan838/Ruhaan838/output/github-contribution-grid-snake.svg)
    </td>
  </tr>
</table>

---

## Fun Fact

> ‚ÄúOptimization is not just speed ‚Äî it's elegance under pressure.‚Äù

---
